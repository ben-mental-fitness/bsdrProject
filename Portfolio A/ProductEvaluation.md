Product Evaluation
===
### summary 
- approach used ( questionnaire)
- gave them access via link, small company, small user group
- how documented insights and feedback from evaluation
- how we used finding to refine final iterations of system

---

### Why Evaluate
We evaluate so we can judge how good each
release is and to systematically evaluate the product to make the necessary improvements required by the client. 
After the initial requirements, some subtle qualities of the system may have been missed either by us or the client, by evaluating 
we ensure the product is what the client envisioned. 
---

### Evaluation Approach 
Our evaluation approach consisted of; questionnaire, observation
<!--giving the client access to regularly deployed updates, meetings, asking questions, questions-->
we wanted to find out... in order to pull out useful knowledge, making evolutionary system changes, iterative process
- Usefulness = utility x usability
    - Utility           
        - System should do what the users need
        - Need to get people to try it out
        - Assessed “in the wild”, in realistic context and environment
     - Usability
        -	Efficient, reliable, satisfying, learnable, maintainable, safe, secure, accessible, memorable, configurable, available

- Tasked-based evaluation
    -	Get real users to try out system functionality 
    -	Give them specific task to achieve using system
    -	Observe how well they get on 
    -	Document areas that need improvement

-	Quantitative and qualitative
        -	Tit – numerically
        -	Lit – measure literally

-	Bad questions
        -	Leading, tries to convince responder
        -	Loaded, put responder in difficult position
        -	Double-barrelled, has more than one element
        -	Absolute, using “always” or “ever”, we can always think of an exception
        -	Irrelevant, something we don’t care about, or can’t do anything about
        -	Truism, of course it is so pointless to ask
        -	Indeterminable, can’t gauge what’s being asked
        -	Indecipherable, hard for respondent to understand question
        -	Vague, doesn’t provide any useful info




-	What do we want to know we have to improve about UI
-	How usable is product
        -	Should meet client’s requirements
        -	Try it out, couple clicks
-	Efficient, reliable, satisfying, learnable, maintainable, safe, secure, accessible, memorable, configurable, available


#####Questionnaire
1.	On a scale of 1-10 how technically minded would you say you are? About how long did it take you to get to grips with MicroStrategy?
1.	Observe how well they get on with figuring out how the system works?
        1.	How many button clicks on average
        1.	How many trace backs
        1.	How many times asking for help/clarification
1.	Where would you have assumed the buttons to export the visualisation would have been?
1.	How well do you think the other BSDR employees (coders) will manage with getting use to this new visualisation software?
1.	On a scale of 1-10 how well does our product translate the features you needed from MicroStrategy?
1.	What features from MicroStrategy may be missing from our product?
1.	Do you have a need to be able to change the text on nodes or edges?


---


### 1st Feedback
<!--first, filtering based on node weight-->

---

### 2nd Feedback
<!--final one, weigh on nodes-->



---

### 